import asyncio
import os
import shutil
from dotenv import load_dotenv

# LangChain / LangGraph å¯¼å…¥
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent

# MCP å®¢æˆ·ç«¯å¯¼å…¥
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_core.tools import tool

load_dotenv()

async def run_agent_demo():
    # 1. é…ç½®æœåŠ¡å™¨è¿æ¥å‚æ•°
    # æˆ‘ä»¬ç›´æ¥æŒ‡å‘åˆšæ‰å†™çš„ server æ–‡ä»¶
    server_params = StdioServerParameters(
        command="python",
        args=["weather_server.py"], # ç¡®ä¿è·¯å¾„æ­£ç¡®
        env=os.environ.copy()
    )

    print("ğŸ”— æ­£åœ¨è¿æ¥åˆ° US Weather MCP æœåŠ¡å™¨...")

    # 2. å»ºç«‹ MCP è¿æ¥å¹¶è½¬æ¢ä¸º LangChain å·¥å…·
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # åˆå§‹åŒ–
            await session.initialize()
            
            # è·å–æœåŠ¡å™¨ä¸Šçš„å·¥å…·åˆ—è¡¨
            tools_list = await session.list_tools()
            print(f"âœ… å‘ç°å·¥å…·: {[t.name for t in tools_list.tools]}")

            # 3. åŠ¨æ€åŒ…è£… MCP å·¥å…·ä¸º LangChain å·¥å…·
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª LangChain å…¼å®¹çš„ Tool åˆ—è¡¨
            langchain_tools = []
            
            for mcp_tool in tools_list.tools:
                # è¿™æ˜¯ä¸€ä¸ªé—­åŒ…ï¼Œç”¨äºæ•è·å½“å‰çš„ tool_name
                async def make_tool_func(t_name=mcp_tool.name):
                    @tool(t_name)
                    async def dynamic_tool(query: str):
                        """Dynamic wrapper for MCP tool"""
                        return await session.call_tool(t_name, arguments={"city_query": query})
                    return dynamic_tool
                
                # æ„å»ºå·¥å…·å®ä¾‹
                lc_tool = await make_tool_func()
                # æ›´æ–°å·¥å…·æè¿°ä»¥åŒ¹é… MCP å®šä¹‰
                lc_tool.description = mcp_tool.description
                langchain_tools.append(lc_tool)

            # 4. æ„å»º LangGraph Agent
            # ä½¿ç”¨ GPT-4o æˆ– GPT-3.5
            # llm = ChatOpenAI(model="gemini-3-pro-preview", temperature=0)
            llm = ChatOpenAI(
                # å•†å®¶æ”¯æŒå¾ˆå¤šæ¨¡å‹ï¼Œä½ å¯ä»¥è¯•ç€ç”¨ "gpt-4o"ï¼Œ
                # æˆ–è€…ç”¨å•†å®¶åˆ—è¡¨é‡Œçš„ "gemini-2.5-pro" (é€šå¸¸æ›´ä¾¿å®œä¸”æ”¯æŒé•¿æ–‡æœ¬)
                model="gpt-4o", 
                temperature=0,
                # ğŸ‘‡ã€æ ¸å¿ƒä¿®æ”¹ã€‘å¿…é¡»åŠ ä¸Šè¿™è¡Œï¼ŒæŒ‡å‘å•†å®¶çš„åœ°å€
                base_url="https://max.openai365.top/v1",
                api_key=os.environ["OPENAI_API_KEY"] # ç¡®ä¿è¿™é‡Œè¯»å–çš„æ˜¯.envé‡Œçš„æ–°key
            )
            
            # create_react_agent è‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨å¾ªç¯
            agent_executor = create_react_agent(llm, langchain_tools)

            print("\nğŸ¤– Agent å·²å°±ç»ªã€‚å¼€å§‹æŸ¥è¯¢...\n")

            # 5. æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•
            # æµ‹è¯•ç”¨ä¾‹ 1: ç®€å•æŸ¥è¯¢
            query1 = "Check the weather in Santa Clara, CA."
            print(f"ç”¨æˆ·: {query1}")
            
            async for chunk in agent_executor.astream(
                {"messages": [HumanMessage(content=query1)]}, 
                stream_mode="values"
            ):
                # æ‰“å°æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
                last_msg = chunk["messages"][-1]
                if last_msg.type == "ai" and last_msg.tool_calls:
                    print(f"ğŸ‘‰ åŠ©æ‰‹å†³å®šè°ƒç”¨å·¥å…·: {last_msg.tool_calls[0]['name']}")
                elif last_msg.type == "tool":
                    print("ğŸ“¦ å·¥å…·è¿”å›æ•°æ® (å·²éšè—è¯¦ç»† JSON)")
                elif last_msg.type == "ai":
                    print(f"ğŸ’¬ åŠ©æ‰‹å›ç­”: {last_msg.content}")

            print("-" * 50)

            # æµ‹è¯•ç”¨ä¾‹ 2: å¤æ‚å¤šåœ°æŸ¥è¯¢
            query2 = "Compare the temperature in New York and Miami right now. Which one is hotter?"
            print(f"\nç”¨æˆ·: {query2}")
            
            async for chunk in agent_executor.astream(
                {"messages": [HumanMessage(content=query2)]}, 
                stream_mode="values"
            ):
                if chunk["messages"][-1].type == "ai" and not chunk["messages"][-1].tool_calls:
                     print(f"ğŸ’¬ åŠ©æ‰‹å›ç­”: {chunk['messages'][-1].content}")

if __name__ == "__main__":
    # ç¡®ä¿æ˜¯åœ¨å½“å‰ç›®å½•ä¸‹è¿è¡Œï¼Œæˆ–è€…ä¿®æ”¹ path
    asyncio.run(run_agent_demo())